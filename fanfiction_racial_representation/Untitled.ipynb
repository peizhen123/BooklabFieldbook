{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--out_csv OUT_CSV] [--header HEADER]\n",
      "                             [--num_to_retrieve NUM_TO_RETRIEVE]\n",
      "                             [--multichapter_only MULTICHAPTER_ONLY]\n",
      "                             [--tag_csv TAG_CSV]\n",
      "                             URL\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "import sys\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "page_empty = False\n",
    "base_url = \"\"\n",
    "url = \"\"\n",
    "num_requested_fic = 0\n",
    "num_recorded_fic = 0\n",
    "csv_name = \"\"\n",
    "multichap_only = \"\"\n",
    "tags = []\n",
    "\n",
    "# keep track of all processed ids to avoid repeats:\n",
    "# this is separate from the temporary batch of ids\n",
    "# that are written to the csv and then forgotten\n",
    "seen_ids = []\n",
    "\n",
    "# \n",
    "# Ask the user for:\n",
    "# a url of a works listed page\n",
    "# e.g. \n",
    "# https://archiveofourown.org/works?utf8=%E2%9C%93&work_search%5Bsort_column%5D=word_count&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=&work_search%5Bcomplete%5D=0&commit=Sort+and+Filter&tag_id=Harry+Potter+-+J*d*+K*d*+Rowling\n",
    "# https://archiveofourown.org/tags/Harry%20Potter%20-%20J*d*%20K*d*%20Rowling/works?commit=Sort+and+Filter&page=2&utf8=%E2%9C%93&work_search%5Bcomplete%5D=0&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=&work_search%5Bsort_column%5D=word_count\n",
    "# how many fics they want\n",
    "# what to call the output csv\n",
    "# \n",
    "# If you would like to add additional search terms (that is should contain at least one of, but not necessarily all of)\n",
    "# specify these in the tag csv, one per row. \n",
    "\n",
    "def get_args():\n",
    "    global base_url\n",
    "    global url\n",
    "    global csv_name\n",
    "    global num_requested_fic\n",
    "    global multichap_only\n",
    "    global tags\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Scrape AO3 work IDs given a search URL')\n",
    "    parser.add_argument(\n",
    "        'url', metavar='URL',\n",
    "        help='a single URL pointing to an AO3 search page')\n",
    "    parser.add_argument(\n",
    "        '--out_csv', default='work_ids',\n",
    "        help='csv output file name')\n",
    "    parser.add_argument(\n",
    "        '--header', default='',\n",
    "        help='user http header')\n",
    "    parser.add_argument(\n",
    "        '--num_to_retrieve', default='a', \n",
    "        help='how many fic ids you want')\n",
    "    parser.add_argument(\n",
    "        '--multichapter_only', default='', \n",
    "        help='only retrieve ids for multichapter fics')\n",
    "    parser.add_argument(\n",
    "        '--tag_csv', default='',\n",
    "        help='provide an optional list of tags; the retrieved fics must have one or more such tags')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    url = args.url\n",
    "    csv_name = str(args.out_csv)\n",
    "    \n",
    "    # defaults to all\n",
    "    if (str(args.num_to_retrieve) is 'a'):\n",
    "        num_requested_fic = -1\n",
    "    else:\n",
    "        num_requested_fic = int(args.num_to_retrieve)\n",
    "\n",
    "    multichap_only = str(args.multichapter_only)\n",
    "    if multichap_only != \"\":\n",
    "        multichap_only = True\n",
    "    else:\n",
    "        multichap_only = False\n",
    "\n",
    "    tag_csv = str(args.tag_csv)\n",
    "    if (tag_csv):\n",
    "        with open(tag_csv, \"r\") as tags_f:\n",
    "            tags_reader = csv.reader(tags_f)\n",
    "            for row in tags_reader:\n",
    "                tags.append(row[0])\n",
    "\n",
    "    header_info = str(args.header)\n",
    "\n",
    "    return header_info\n",
    "\n",
    "# \n",
    "# navigate to a works listed page,\n",
    "# then extract all work ids\n",
    "# \n",
    "def get_ids(header_info=''):\n",
    "    global page_empty\n",
    "    headers = {'user-agent' : header_info}\n",
    "    req = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(req.text, \"lxml\")\n",
    "\n",
    "    # some responsiveness in the \"UI\"\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    works = soup.select(\"li.work.blurb.group\")\n",
    "    # see if we've gone too far and run out of fic: \n",
    "    if (len(works) is 0):\n",
    "        page_empty = True\n",
    "\n",
    "    # process list for new fic ids\n",
    "    ids = []\n",
    "    for tag in works:\n",
    "        if (multichap_only):\n",
    "            # FOR MULTICHAP ONLY\n",
    "            chaps = tag.find('dd', class_=\"chapters\")\n",
    "            if (chaps.text != u\"1/1\"):\n",
    "                t = tag.get('id')\n",
    "                t = t[5:]\n",
    "                if not t in seen_ids:\n",
    "                    ids.append(t)\n",
    "                    seen_ids.append(t)\n",
    "        else:\n",
    "            t = tag.get('id')\n",
    "            t = t[5:]\n",
    "            if not t in seen_ids:\n",
    "                ids.append(t)\n",
    "                seen_ids.append(t)\n",
    "    return ids\n",
    "\n",
    "# \n",
    "# update the url to move to the next page\n",
    "# note that if you go too far, ao3 won't error, \n",
    "# but there will be no works listed\n",
    "# \n",
    "def update_url_to_next_page():\n",
    "    global url\n",
    "    key = \"page=\"\n",
    "    start = url.find(key)\n",
    "\n",
    "    # there is already a page indicator in the url\n",
    "    if (start is not -1):\n",
    "        # find where in the url the page indicator starts and ends\n",
    "        page_start_index = start + len(key)\n",
    "        page_end_index = url.find(\"&\", page_start_index)\n",
    "        # if it's in the middle of the url\n",
    "        if (page_end_index is not -1):\n",
    "            page = int(url[page_start_index:page_end_index]) + 1\n",
    "            url = url[:page_start_index] + str(page) + url[page_end_index:]\n",
    "        # if it's at the end of the url\n",
    "        else:\n",
    "            page = int(url[page_start_index:]) + 1\n",
    "            url = url[:page_start_index] + str(page)\n",
    "\n",
    "    # there is no page indicator, so we are on page 1\n",
    "    else:\n",
    "        # there are other modifiers\n",
    "        if (url.find(\"?\") is not -1):\n",
    "            url = url + \"&page=2\"\n",
    "        # there an no modifiers yet\n",
    "        else:\n",
    "            url = url + \"?page=2\"\n",
    "\n",
    "\n",
    "# modify the base_url to include the new tag, and save to global url\n",
    "def add_tag_to_url(tag):\n",
    "    global url\n",
    "    key = \"&work_search%5Bother_tag_names%5D=\"\n",
    "    if (base_url.find(key)):\n",
    "        start = base_url.find(key) + len(key)\n",
    "        new_url = base_url[:start] + tag + \"%2C\" + base_url[start:]\n",
    "        url = new_url\n",
    "    else:\n",
    "        url = base_url + \"&work_search%5Bother_tag_names%5D=\" + tag\n",
    "\n",
    "\n",
    "# \n",
    "# after every page, write the gathered ids\n",
    "# to the csv, so a crash doesn't lose everything.\n",
    "# include the url where it was found,\n",
    "# so an interrupted search can be restarted\n",
    "# \n",
    "def write_ids_to_csv(ids):\n",
    "    global num_recorded_fic\n",
    "    with open(csv_name + \".csv\", 'a') as csvfile:\n",
    "        wr = csv.writer(csvfile, delimiter=',')\n",
    "        for id in ids:\n",
    "            if (not_finished()):\n",
    "                wr.writerow([id, url])\n",
    "                num_recorded_fic = num_recorded_fic + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "# \n",
    "# if you want everything, you're not done\n",
    "# otherwise compare recorded against requested.\n",
    "# recorded doesn't update until it's actually written to the csv.\n",
    "# If you've gone too far and there are no more fic, end. \n",
    "# \n",
    "def not_finished():\n",
    "    if (page_empty):\n",
    "        return False\n",
    "\n",
    "    if (num_requested_fic == -1):\n",
    "        return True\n",
    "    else:\n",
    "        if (num_recorded_fic < num_requested_fic):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# \n",
    "# include a text file with the starting url,\n",
    "# and the number of requested fics\n",
    "# \n",
    "def make_readme():\n",
    "    with open(csv_name + \"_readme.txt\", \"w\") as text_file:\n",
    "        text_file.write(\"url: \" + url + \"\\n\" + \"num_requested_fic: \" + str(num_requested_fic) + \"\\n\" + \"retreived on: \" + str(datetime.datetime.now()))\n",
    "\n",
    "# reset flags to run again\n",
    "# note: do not reset seen_ids\n",
    "def reset():\n",
    "    global page_empty\n",
    "    global num_recorded_fic\n",
    "    page_empty = False\n",
    "    num_recorded_fic = 0\n",
    "\n",
    "def process_for_ids(header_info=''):\n",
    "    while(not_finished()):\n",
    "        # 5 second delay between requests as per AO3's terms of service\n",
    "        time.sleep(5)\n",
    "        ids = get_ids(header_info)\n",
    "        write_ids_to_csv(ids)\n",
    "        update_url_to_next_page()\n",
    "\n",
    "def main():\n",
    "    header_info = get_args()\n",
    "    make_readme()\n",
    "\n",
    "    print (\"processing...\\n\")\n",
    "\n",
    "    if (len(tags)):\n",
    "        for t in tags:\n",
    "            print (\"Getting tag: \", t)\n",
    "            reset()\n",
    "            add_tag_to_url(t)\n",
    "            process_for_ids(header_info)\n",
    "    else:\n",
    "        process_for_ids(header_info)\n",
    "\n",
    "    print (\"That's all, folks.\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
